What prompts were used
- we prompted one LLM to discuss a topic from the perspective of the article to another LLM in a conversation style, and after each answer measure the other model's political alignment using a political compass test.
What kind of responses were received
- results of the political compass test in terms of coordinates at each stage of the conversation, along with the responses themselves.
How might you improve your experiment?
- develop a more refined political compass test (current one is open sourced) 
What variables do you intend to vary?
- we intend to vary the LLM itself, Article source/topic, and number of rounds of conversation
How will you expand on your starting experiment?
- more article topics, more analysis on how the political alignment shifted throughout the conversation
How might you automate largescale data collection?
- for gathering articles for prompting, removing non-unicode characters.

Python code to run prompts through a model
- src/proof_of_concept.py
Saved output from tests
- output.json
Code to read/analyze the output to ensure variety
