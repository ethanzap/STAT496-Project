What prompts were used
- we prompted one LLM to discuss a topic from the perspective of the article to another LLM in a conversation style, and after each answer measure the other model's political alignment using a political compass test.
- we prompted it on articles on the same issue but different sources
What kind of responses were received
- results of the political compass test in terms of coordinates at each stage of the conversation, along with the responses themselves.
- looking at the plots of the political alignment movement, the llm remained in the third quadrant of the political compass for all 6 articles.
- the political trajectory on the plot is different for each source. 

How might you improve your experiment?
- develop a more refined political compass test (current one is open sourced) 
- improve clarity and interpretability of output.
What variables do you intend to vary?
- we intend to vary the LLM itself, Article source/topic, and number of rounds of conversation
How will you expand on your starting experiment?
- more article topics (most important)
- more analysis on how the political alignment shifted throughout the conversation
- 
How might you automate largescale data collection?
- for gathering articles for prompting, removing non-unicode characters.

Python code to run prompts through a model
- src/proof_of_concept.py
Saved output from tests
- output.json
Code to read/analyze the output to ensure variety
- plots folder
